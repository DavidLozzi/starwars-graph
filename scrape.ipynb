{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres\n",
    "\n",
    "Stores my raw data and final prepped data hosted at 192.168.68.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql, pool\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'database': 'star_wars_data',\n",
    "    'user': 'postgres',\n",
    "    'password': password,\n",
    "    'host': '192.168.68.40',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "conn_pool = psycopg2.pool.SimpleConnectionPool(1, 1000, **db_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloaded 223 URLs\n"
     ]
    }
   ],
   "source": [
    "def run_sql(sql_command, data=None):\n",
    "    try:\n",
    "        conn = conn_pool.getconn()\n",
    "        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "        cursor = conn.cursor()\n",
    "        if data is not None:\n",
    "            # print(\"data is not None\")\n",
    "            cursor.execute(sql_command, data)\n",
    "        else:\n",
    "            # print(\"data is None\")\n",
    "            cursor.execute(sql_command)\n",
    "        result = cursor.fetchall()\n",
    "        # print(result)\n",
    "        cursor.close()\n",
    "\n",
    "        # print(\"psql completed\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"A database error occurred: {e}\")\n",
    "    finally:\n",
    "        conn_pool.putconn(conn)\n",
    "\n",
    "checked_urls = set()\n",
    "def url_exists(url):\n",
    "    if url in checked_urls:\n",
    "        return True\n",
    "    select_query = \"SELECT URL FROM all_data WHERE URL = %s\"\n",
    "    result = run_sql(select_query, (url,))\n",
    "    if len(result) > 0:\n",
    "        checked_urls.add(url)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def preload_checked_urls():\n",
    "    global checked_urls\n",
    "    select_query = \"SELECT URL FROM all_data\"\n",
    "    result = run_sql(select_query)\n",
    "    checked_urls = set()\n",
    "    for row in result:\n",
    "        checked_urls.add(row[0])\n",
    "    print(f\"Preloaded {len(checked_urls)} URLs\")\n",
    "\n",
    "preload_checked_urls()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "base_url = \"https://starwars.fandom.com/\"\n",
    "start_url = \"https://starwars.fandom.com/sitemap-newsitemapxml-index.xml\"\n",
    "\n",
    "\n",
    "ignore_urls = [\n",
    "    \"/wiki/Special:\",\n",
    "    \"/wiki/User_talk:\",\n",
    "    \"/wiki/Template:\",\n",
    "    \"/wiki/Template_talk:\",\n",
    "    \"/wiki/Help:\",\n",
    "    \"/wiki/User:\",\n",
    "    \"/wiki/UserProfile:\",\n",
    "    '/register',\n",
    "    '/signin',\n",
    "    '/reset-password']\n",
    "\n",
    "# get updates from https://starwars.fandom.com/wiki/Special:Log\n",
    "preload_checked_urls()\n",
    "\n",
    "total_processed = 0\n",
    "total_added = 0\n",
    "total_skipped = 0\n",
    "errors_urls = []\n",
    "async def fetch(url):\n",
    "    global errors_urls\n",
    "    try:\n",
    "        async with httpx.AsyncClient(follow_redirects=True, max_redirects=20) as client:\n",
    "            resp = await client.get(url)\n",
    "            if resp.status_code == 200:\n",
    "                content_type = resp.headers.get('Content-Type', '')\n",
    "                if 'xml' in content_type:\n",
    "                    return resp, 'xml'\n",
    "                elif 'html' in content_type:\n",
    "                    return resp, 'html'\n",
    "                else:\n",
    "                    print(f\"Unknown content type for {url}: {content_type}\")\n",
    "                    return resp, 'html'\n",
    "\n",
    "            else:\n",
    "                raise Exception(f\"{url} {resp.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GET error occurred: {e}\")\n",
    "        errors_urls.append((url,e))\n",
    "        return None\n",
    "\n",
    "async def scrape(url, base_url):\n",
    "    global total_processed\n",
    "    global total_added\n",
    "    global total_skipped\n",
    "    print(f\"\\nTotal checked: {len(checked_urls)}, processed: {total_processed} in {time.time() - start_time}s (avg {total_processed/(time.time() - start_time)}/s), added: {total_added}, skipped: {total_skipped}\")\n",
    "    print(\"  Processing:\", url)\n",
    "    response, content_type = await fetch(url)\n",
    "    # await asyncio.sleep(1)\n",
    "    if response is None:\n",
    "        return\n",
    "\n",
    "    links = []\n",
    "    if content_type == 'xml':\n",
    "        soup = BeautifulSoup(response.text, 'lxml-xml')\n",
    "        links = [loc.text for loc in soup.find_all('loc')]\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        body = soup.find('body')\n",
    "\n",
    "        h1_tags = body.find_all(id='firstHeading')\n",
    "        title = h1_tags[0].text.strip() if h1_tags else \"No title\"\n",
    "        content = str(body).replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\r\",\"\")\n",
    "\n",
    "        store_data(title, url, content)\n",
    "\n",
    "        total_processed += 1\n",
    "\n",
    "        # links = [urljoin(base_url, link.get('href')) for link in soup.find_all('a')]\n",
    "\n",
    "    tasks = []\n",
    "    my_urls = set() # a single page will have the same URL many times\n",
    "    for link in links:\n",
    "        url_clean = link\n",
    "        if '?' in url_clean:\n",
    "            url_clean = url_clean[:url_clean.index('?')]\n",
    "        url_clean = url_clean if len(url_clean) <= 250 else url_clean[:250]\n",
    "        if url_clean not in my_urls:\n",
    "            my_urls.add(url_clean)\n",
    "            if not any(ignore_url in url_clean for ignore_url in ignore_urls) and \\\n",
    "              url_clean != url and \\\n",
    "              url_exists(url_clean) == False and \\\n",
    "              base_url in url_clean and \\\n",
    "              base_url != url_clean:\n",
    "                tasks.append(scrape(url_clean, base_url))\n",
    "                # await asyncio.sleep(0.5)\n",
    "                # await scrape(url_clean, base_url)\n",
    "            else:\n",
    "                total_skipped += 1\n",
    "\n",
    "    soup.decompose()\n",
    "    soup=None\n",
    "\n",
    "    print(f\"Gathering tasks for {url}, count: {len(tasks)}\")\n",
    "    await asyncio.gather(*tasks)\n",
    "    print(f\"Finished processing {url}\")\n",
    "    print(f\"Elapsed time: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "\n",
    "def store_data(title, url, content):\n",
    "    global total_added\n",
    "    if not url_exists(url):\n",
    "        postgres_insert_query = \"INSERT INTO all_data (Title, URL, Content, Last_Ingested) VALUES (%s,%s,%s,%s) RETURNING *\"\n",
    "        record_to_insert = (title, url, content, datetime.datetime.now())\n",
    "        run_sql(postgres_insert_query, record_to_insert)\n",
    "        print('  Added URL:', url)\n",
    "        total_added += 1\n",
    "        checked_urls.add(url)\n",
    "    else:\n",
    "        print(\"  URL already exists in database\", url)\n",
    "\n",
    "start_time = time.time()\n",
    "await scrape(start_url, base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(err[0]) for err in errors_urls])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_clean = \"https://starwars.fandom.com/wiki/Main_Page\"\n",
    "if '?' in url_clean:\n",
    "    url_clean = url_clean[:url_clean.index('?')]\n",
    "url_clean = url_clean if len(url_clean) <= 250 else url_clean[:250]\n",
    "print(url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n"
     ]
    }
   ],
   "source": [
    "select_query = \"SELECT URL FROM all_data\"\n",
    "result = run_sql(select_query)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
